<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Temporally Consistent Video Editing</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Temporally Consistent Object Editing in Videos using Extended Attention</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/amirhosseinzamani/" target="_blank">AmirHossein Zamani</a><sup>1,2*</sup>,</span>
                <span class="author-block">
                  <a href="https://users.encs.concordia.ca/~aghdam/" target="_blank">Amir G. Aghdam</a><sup>1</sup>,</span>
                  <span class="author-block">
                    <a href="https://users.encs.concordia.ca/~stpopa/" target="_blank">Tiberiu Popa</a><sup>1</sup>,</span>
                    <span class="author-block">
                      <a href="https://mila.quebec/en/directory/eugene-belilovsky" target="_blank">Eugene Belilovsky</a><sup>1,2</sup>,</span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Concordia University, Montreal, Canada; <sup>2</sup>Mila â€“ Quebec AI Institute <br><h1>IEEE/CFV CVPR Workshop on AI for Content Creation (AI4CC) 2024</h1></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Correspondence Author</small></span>                    
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2406.00272" target="_blank" class="external-link button is-normal is-rounded is-dark">
                          <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                          </span>
                          <span>Paper</span>
                        </a>
                      </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/AHHHZ975/TemporallyConsistentVideoEditing" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>

                  <!-- Poster link -->
                  <span class="link-block">
                    <a href="https://drive.google.com/file/d/1yLyQrmniZVN7FtcVtcVw-z7DXM7z1UJ5/view?usp=sharing" target="_blank" class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Poster</span>
                    </a>
                  </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/c31yfdakG4w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->  


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
                Image generation and editing have seen a great deal of advancements with the rise of large-scale diffusion models that allow user control of different modalities
                such as text, mask, depth maps, etc. However, controlled editing of videos still lags behind. Prior work in this area has focused on using 2D diffusion models to
                globally change the style of an existing video. On the other hand, in many practical applications, editing localized parts of the video is critical. In this work,
                we propose a method to edit videos using a pre-trained inpainting image diffusion model. We systematically redesign the forward path of the model by replacing the
                self-attention modules with an extended version of attention modules that creates frame-level dependencies. In this way, we ensure that the edited information will
                be consistent across all the video frames no matter what the shape and position of the masked area is. We qualitatively compare our results with state-of-the-art in
                terms of accuracy on several video editing tasks like object retargeting, object replacement, and object removal tasks. Simulations demonstrate the superior performance
                of the proposed strategy.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methodology</h2>
        <div class="content has-text-justified">
          <p>
                We systematically redesign the forward path of the pre-trained inpainting diffusion model by replacing the self-attention 
                modules with an extended version of attention modules that induces dependencies between frames. Note that we do not change
                the architecture of the existing U-Net in the SD model. We manipulate only the computation in the forward path of the self-attention layers.
                This happens by using several frames instead of one in the computation of self-attention modules to extract similar information or features.
                That is why our approach does not add any additional training or fine-tuning. Then, having these extracted similar features, we enforce the
                model to edit (reconstruct) the video in a way that the regions in the frames that have similar features will be kept unchanged while the other
                parts of the frames will be changed according to the control commands (mask and text prompts). In this way, we ensure that the edited information
                will be consistent across all the video frames no matter what the shape and position of the masked area is. \cref{fig:DiffusionProcess} (right)
                demonstrates a visual representation of how the forward path of the extended attention works in our framework. \cref{fig:DiffusionProcess} (left)
                shows the whole diffusion process of our temporal consistent video editing technique. More specifically, for each diffusion step, similar to \cite{TokenFlow},
                we randomly select several frames and their corresponding mask images. Then, these pairs of masks and images are fed into a pre-processor algorithm
                explained above. Then, the extended attention layers in the U-Net architecture, showing in \cref{fig:DiffusionProcess}, extract similar features from
                the selected frames. This process is repeated for $T=50$ diffusion steps.
          </p>
          <img src="static/images/Diffusion_inpainting_process.png" alt="An overview of the diffusion process for temporal consistent video editing" class="blend-img-background center-image">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      We appreciate your interest in our research. If you want to use our work, please consider the proper citation format written below.
      <pre><code>@article{zamani2024temporally,
  title={Temporally Consistent Object Editing in Videos using Extended Attention},
  author={Zamani, AmirHossein and Aghdam, Amir G and Popa, Tiberiu and Belilovsky, Eugene},
  journal={arXiv preprint arXiv:2406.00272},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
